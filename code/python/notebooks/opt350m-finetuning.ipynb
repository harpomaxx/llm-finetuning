{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning OPT350m on a single GPU\n",
    "\n",
    "**GOAL:** fine tune opt350m for code assistance \n",
    "**Method:**  train on response only using code-alpaca-20k dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpo/miniconda3/envs/fastchat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from trl import SFTTrainer,DataCollatorForCompletionOnlyLM\n",
    "from pynvml import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful functions for analyzing the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 237 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and Bytes configuration for using quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"facebook/opt-350m\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and write the code to define the first four number elements a given number of times, or the definition of the last 4 number elements a given number of times (or the third number element is an infinite number of times). The code can be used to compute the prime numbers independently for arbitrary values of the number; for example, if the function returns a single prime number, the calculation is performed by using the value of the first four number element of the first parameter alone, and only two prime numbers are required for the computation (i.e., the first 4 number element).\n",
      "In one preferred embodiment, the function to calculate prime numbers may be a function to determine how many prime numbers a given number of times are included in a set of prime numbers. This function also preferably has the capability of calculating the prime numbers and prime numbers in successive numbers, and calculating the prime numbers and prime numbers in successive numbers independently of each other.\n",
      "In another preferred embodiment, the function to calculate prime numbers may first determine what prime number number and prime number are included in each parameter of the function, and then calculate one prime number and prime number independently. The function may then further determine where in the table the prime numbers are located. In this way,\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import TextStreamer, pipeline\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "set_seed(32)\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model, \n",
    "                     tokenizer=tokenizer, \n",
    "                     streamer = streamer, \n",
    "                     do_sample=True,\n",
    "                     max_length= 256)\n",
    "prompt = \"\"\"Code a function for calculating prime numbers\"\"\"\n",
    "_=generator(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRa Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 335,915,008 || trainable%: 1.4046981788917272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.5,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\"] # obtained by the output of the model\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"../../../results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    optim='adamw_bnb_8bit',\n",
    "    save_steps=250,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=0.3,\n",
    "    #max_steps=5000,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/harpo/.cache/huggingface/datasets/lucasmccabe-lmi___parquet/lucasmccabe-lmi--CodeAlpaca-20k-b92d1194a2c963a0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "#train_dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Update the existing code to have a shape of (3,3) when an array of size (2,2) is reshaped. Write corresponding code in Python.',\n",
       " 'input': 'import numpy as np\\n\\narr = np.arange(4).reshape(2,2)',\n",
       " 'output': 'import numpy as np\\n\\narr = np.arange(9).reshape(3,3)'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:-1]['instruction'][1]\n",
    "train_dataset[:-1]['instruction'][1]\n",
    "\n",
    "train_dataset[10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formating function for codealpaca\n",
    "\n",
    "The training is done in a way similar to llama-2 paper were loss is only calculated on the answer (completion). However, do to HF trl limitation, no packing is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"### Question: Generate a unique 8 character string that contains a lowercase letter, an uppercase letter, a numerical digit, and a special character. Write corresponding code in Python.\\n ### Answer: import string\\nimport random\\n\\ndef random_password_string():\\n    characters = string.ascii_letters + string.digits + string.punctuation\\n    password = ''.join(random.sample(characters, 8))\\n    return password\\n\\nif __name__ == '__main__':\\n    print(random_password_string())\",\n",
       " '### Question: Given a course consisting of 5 lessons, write a program to display the first lesson. Write corresponding code in Python.\\n ### Answer: def display_first_lesson(lessons):\\n    print(\"The first lesson is: \" + lessons[0])\\n\\nLessons = [\"Python Basics\", \"Javascript Basics\", \"C++ Basics\", \"Data Structures\", \"Object-Oriented Programming\"]\\n\\ndisplay_first_lesson(Lessons)',\n",
       " \"### Question: Create an algorithm to encourage work balance and productivity in remote work.\\n ### Answer: One algorithm to encourage work balance and productivity in remote work is a scheduling algorithm that assigns workloads over manageable working windows with rest periods in between to allow for breaks and recovery. This algorithm can be designed to set maximum and minimum limit for each allocated workload and for duration for the rest periods, based on the individual's productive and rest needs. It can be used to monitor and manage workloads to ensure that workers do not experience fatigue or workload overload. Additionally, the algorithm can be designed to keep track of work performance over time to provide appropriate feedback and recommendations in order to help workers optimize their performance.\",\n",
       " '### Question: Write a JavaScript that changes the text of a paragraph element, from \"Hello, World!\" to \"Goodbye, World!\"\\n ### Answer: document.getElementById(\"myParagraph\").innerHTML = \"Goodbye, World!\";',\n",
       " '### Question: Implement a sorting algorithm which takes an array of integers as input and sorts it in ascending order.\\n ### Answer: def sorting_algorithm(arr):\\n    for i in range(len(arr)-1):\\n        for j in range(i+1, len(arr)):\\n            if arr[i] > arr[j]:\\n                arr[i], arr[j] = arr[j], arr[i]\\n    return arr\\n\\n# Test\\narr = [34, 19, 42, -9, 2018, 0, 105]\\nprint(sorting_algorithm(arr)) # [-9, 0, 19, 34, 42, 105, 2018]',\n",
       " '### Question: Generate a C code snippet to print a given string with a width of 20 characters. Write corresponding code in Python.\\n ### Answer: #include <stdio.h> \\n#include <string.h> \\n  \\nint main(){ \\n    char str[20] = \"Hello\"; \\n    printf(\"%-20s\\\\n\", str); \\n    return 0;\\n}',\n",
       " '### Question: Construct a loop in Swift to find duplicate numbers in an array. Write corresponding code in Python.\\n ### Answer: func FindDuplicates(arr: [Int])  -> [Int] {\\n\\tvar seenNumbers = Set<Int>()\\n\\tvar duplicateValues = [Int]()\\n \\n\\tfor number in arr {\\n\\t\\tif seenNumbers.contains(number) {\\n\\t\\t\\tduplicateValues.append(number)\\n\\t\\t} else {\\n\\t\\t\\tseenNumbers.insert(number)\\n\\t\\t}\\n\\t}\\n \\n\\treturn duplicateValues\\n}',\n",
       " '### Question: Use an object oriented approach to create a class in Java to store the cost and name of an item.\\n ### Answer: class Item { \\n    private String name; \\n    private double cost;\\n \\n    // ... setter and getter methods\\n}',\n",
       " '### Question: Write a Java program that passes an array to a method and prints its length.\\n ### Answer: public class TestArrayLength {\\n    public static void printLength(int[] array) {\\n        System.out.println(\"The length of the array is: \" + array.length);\\n    }\\n    public static void main(String[] args) {\\n        int[] numbers = {1, 2, 3, 4, 5};\\n        printLength(numbers);\\n    }\\n}']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_examples = formatting_prompts_func(train_dataset[1:10])\n",
    "some_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForCompletionOnlyLM(tokenizer=GPT2TokenizerFast(name_or_path='facebook/opt-350m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"opt350m-codealpaca-20k2\"\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    #dataset_text_field=\"text\",\n",
    "    formatting_func= formatting_prompts_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    data_collator= collator\n",
    ")\n",
    "\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpo/miniconda3/envs/fastchat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import TextStreamer, pipeline\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"facebook/opt-350m\",\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "# You can comment and un comment this line to either use base model \n",
    "# or the peft model during the inference.\n",
    "model = PeftModel.from_pretrained(model,'../../../models/',local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "set_seed(32)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model, \n",
    "                     tokenizer=tokenizer, \n",
    "                     streamer = streamer, \n",
    "                     do_sample=True,\n",
    "                     max_length= 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " def prime_ numbers(): \n",
      "  return 0, 1, 2, 3, 4, 5, 6\n",
      "  end\n",
      "\n",
      "# print number result\n",
      "\n",
      "total = (total #1, total #2, total #3, total #4) + 1\n",
      "pipys = []\n",
      "result = n1, result = pips2, result_placement = kpips1, result_value = kpips2>= kpips2 \n",
      "\n",
      "print(result) # print (0) \n",
      "# print (1) \n",
      "# print (2) \n",
      "print(pipys) # print (3) \n",
      "\n",
      "# print (4) \n",
      "print(result_placement) # print (5) \n",
      "console.log(result) # Output: \n",
      "\"1\" \n",
      "console.log(result) # Output: \"8\"\n",
      "\n",
      "# print (5) \n",
      "console.log(result_value) # Output: \"11\"\n",
      " \n",
      "# print (6)\n",
      "console.log(result_value) # Output: \"13\"\n",
      " \n",
      "pipys_input = \"1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Code a function in python for calculating prime numbers ### Answer:\"\"\"\n",
    "_=generator(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface.from_pipeline(generator)\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"../../../models/opt350m-codealpaca-20k/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpo/miniconda3/envs/fastchat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments)\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import TextStreamer, pipeline\n",
    "\n",
    "model_code = AutoModelForCausalLM.from_pretrained(\n",
    "        \"../../../models/opt350m-codealpaca-20k/\",\n",
    "        local_files_only= True,\n",
    "        #quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
      "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(32)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model_code, \n",
    "                     tokenizer=tokenizer, \n",
    "                     streamer = streamer, \n",
    "                     do_sample=True,\n",
    "                     max_length= 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " def prime_ numbers(): \n",
      "  return 0, 1, 2, 3, 4, 5, 6\n",
      "  end\n",
      "\n",
      "# print number result\n",
      "\n",
      "total = (total #1, total #2, total #3, total #4) + 1\n",
      "pipys = []\n",
      "result = n1, result = pips2, result_placement = kpips1, result_value = kpips2>= kpips2 \n",
      "\n",
      "print(result) # print (0) \n",
      "# print (1) \n",
      "# print (2) \n",
      "print(pipys) # print (3) \n",
      "\n",
      "# print (4) \n",
      "print(result_placement) # print (5) \n",
      "console.log(result) # Output: \n",
      "\"1\" \n",
      "console.log(result) # Output: \"8\"\n",
      "\n",
      "# print (5) \n",
      "console.log(result_value) # Output: \"11\"\n",
      " \n",
      "# print (6)\n",
      "console.log(result_value) # Output: \"13\"\n",
      " \n",
      "pipys_input = \"1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Code a function in python for calculating prime numbers ### Answer:\"\"\"\n",
    "_=generator(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "\n",
    "[1] https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91\n",
    "\n",
    "[2] https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only\n",
    "\n",
    "[3] https://huggingface.co/facebook/opt-350m\n",
    "\n",
    "[4] https://medium.com/@rohit.pegallapati/fine-tune-falcon-7b-instruct-model-on-single-commodity-gpu-cf65a86c043a\n",
    "\n",
    "[5] https://huggingface.co/docs/transformers/v4.23.1/en/perf_train_gpu_one\n",
    "\n",
    "[6] [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)\n",
    "\n",
    "[7] https://huggingface.co/docs/peft/conceptual_guides/\n",
    "\n",
    "[8] [Llama 2 paper](https://arxiv.org/pdf/2307.09288.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
